{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGZMO4TGtpoC"
      },
      "source": [
        "# YOLOv5 Instance Segmentation Tutorial\n",
        "\n",
        "YOLOv5 supports instance segmentation tasks too. This is the official YOLOv5 instance segmentation notebook tutorial. YOLOv5 is maintained by [Ultralytics](https://github.com/ultralytics/yolov5).\n",
        "\n",
        "This notebook covers:\n",
        "\n",
        "*   Inference with out-of-the-box YOLOv5 instance segmentation on COCO-128 Segmentatation\n",
        "*  [Training YOLOv5 instance segmentation](https://blog.roboflow.com//train-yolov5-instance-segmentation-custom-dataset) on custom data\n",
        "\n",
        "*Looking for custom data? Explore over 66M community datasets on [Roboflow Universe](https://universe.roboflow.com).*\n",
        "\n",
        "This notebook was created with Google Colab. [Click here](https://colab.research.google.com/drive/1JTz7kpmHsg-5qwVz2d2IH3AaenI1tv0N?usp=sharing) to run it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOrS8kLhtpoE"
      },
      "source": [
        "# Setup\n",
        "Pull in respective libraries to prepare the notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS4FLRHuwmjN",
        "outputId": "12b5c2a7-29c6-4346-a2ca-a08f58e13f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-388-g882c35fc Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 32.7/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git # clone\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TYD7ZstpoF"
      },
      "source": [
        "# 1. Infer on COCO-128\n",
        "\n",
        "To demonstrate YOLOv5 instance segmentation, we'll leverage an already trained model. In this case, we'll download the COCO-128 trained models pretrained on COCO-128 using YOLOv5 Utils.\n",
        "\n",
        "If you'd like to skip to custom trainining, jump to section 3 below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgwmpFOawx6c",
        "outputId": "69fbaef7-a84e-4fac-fed1-33da6e673cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt to weights/yolov5n-seg.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.11M/4.11M [00:00<00:00, 80.9MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt to weights/yolov5s-seg.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.9M/14.9M [00:00<00:00, 79.3MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt to weights/yolov5m-seg.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.4M/42.4M [00:00<00:00, 95.0MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt to weights/yolov5l-seg.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91.9M/91.9M [00:01<00:00, 90.8MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt to weights/yolov5x-seg.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:01<00:00, 106MB/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from utils.downloads import attempt_download\n",
        "\n",
        "p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
        "cls = [f'{x}-seg' for x in p5]  # segmentation models\n",
        "\n",
        "for x in cls:\n",
        "    attempt_download(f'weights/yolov5{x}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_O-19ZtpoG"
      },
      "source": [
        "Now, we can infer on an example image from the COCO-128 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mLJ9V67px8AO"
      },
      "outputs": [],
      "source": [
        "#Download example image\n",
        "import requests\n",
        "image_url = \"https://i.imgur.com/EbOBS5l.jpg\"\n",
        "img_data = requests.get(image_url).content\n",
        "with open('zebra.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "QwheIrVZ0mAl",
        "outputId": "efec997f-18e9-4d48-967b-294b80769f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1msegment/predict: \u001b[0mweights=['./weights/yolov5s-seg.pt'], source=zebra.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/predict-seg, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1, retina_masks=False\n",
            "YOLOv5 ðŸš€ v7.0-388-g882c35fc Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s-seg summary: 224 layers, 7611485 parameters, 0 gradients, 26.4 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/segment/predict.py\", line 307, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/segment/predict.py\", line 302, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/segment/predict.py\", line 135, in run\n",
            "    for path, im, im0s, vid_cap, s in dataset:\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 396, in __next__\n",
            "    im0 = cv2.imread(path)  # BGR\n",
            "  File \"/content/yolov5/utils/general.py\", line 1274, in imread\n",
            "    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n",
            "cv2.error: OpenCV(4.10.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:813: error: (-215:Assertion failed) !buf.empty() in function 'imdecode_'\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'runs/predict-seg/exp/zebra.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-505634b99330>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Infer using segment/predict.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python segment/predict.py --weights ./weights/yolov5s-seg.pt --source zebra.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/predict-seg/exp/zebra.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/predict-seg/exp/zebra.jpg'"
          ]
        }
      ],
      "source": [
        "#Infer using segment/predict.py\n",
        "!python segment/predict.py --weights ./weights/yolov5s-seg.pt --source zebra.jpg\n",
        "display.Image(filename='runs/predict-seg/exp/zebra.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYgLUWYdtpoH"
      },
      "source": [
        "## 2. (Optional) Validate\n",
        "\n",
        "Use the `segment/val.py` script to run validation for the model. This will show us the model's performance.\n",
        "\n",
        "First, we need to download COCO-128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykGYRklD_Bqn",
        "outputId": "9d8824d7-33c9-44a5-d5e3-2578cb8068fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.79M/6.79M [00:00<00:00, 122MB/s]\n"
          ]
        }
      ],
      "source": [
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco128-seg.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQUhbqy7FfpG",
        "outputId": "7ae22cb9-1f76-4fef-9ee9-7ec9baf37e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/content/yolov5/data/coco128-seg.yaml, weights=['yolov5s-seg.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val-seg, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v7.0-388-g882c35fc Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt to yolov5s-seg.pt...\n",
            "100% 14.9M/14.9M [00:00<00:00, 203MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s-seg summary: 224 layers, 7611485 parameters, 0 gradients, 26.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128-seg/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<00:00, 729.88it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco128-seg/labels/train2017.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 4/4 [00:14<00:00,  3.63s/it]\n",
            "                   all        128        929      0.693       0.65      0.712      0.488      0.675      0.626      0.665      0.405\n",
            "Speed: 1.2ms pre-process, 9.4ms inference, 15.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val-seg/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python segment/val.py --weights yolov5s-seg.pt --data coco128-seg.yaml --img 640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgX0LMDtpoI"
      },
      "source": [
        "The output shows performance metrics for the COCO-128 validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEFcAs9wtpoI"
      },
      "source": [
        "# 3. Train On Custom Data\n",
        "\n",
        "To train on custom data, we need to prepare a dataset with custom labels.\n",
        "\n",
        "To prepare custom data, we'll use [Roboflow](https://roboflow.com). Roboflow enables easy dataset prep with your team, including labeling, formatting into the right export format, deploying, and active learning with a `pip` package.\n",
        "\n",
        "If you need custom data, there are over 66M open source images from the community on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "(For more guidance, here's a detailed blog on [training YOLOv5 instance segmentation on custom data](https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset).)\n",
        "\n",
        "\n",
        "Create a free Roboflow account, upload your data, and label.\n",
        "\n",
        "![](https://robflow-public-assets.s3.amazonaws.com/how-to-train-yolov5-segmentation-annotation.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0BH738HtpoI"
      },
      "source": [
        "### Load Custom Dataset\n",
        "\n",
        "Next, we'll export our dataset into the right directory structure for training YOLOv5 segmentation to load into this notebook. Select the `Export` button at the top of the version page, `YOLO v5 Pytorch` type, and `show download code`.\n",
        "\n",
        "This ensures all our directories are in the right format with the needed data.yaml file:\n",
        "\n",
        "```\n",
        "dataset\n",
        "â”œâ”€â”€ train\n",
        "â”‚Â Â  â”œâ”€â”€ images\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_123.jpg\n",
        "â”‚Â Â  â””â”€â”€ labels\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_123.txt\n",
        "â”œâ”€â”€ valid\n",
        "â”‚Â Â  â”œâ”€â”€ images\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_456.jpg\n",
        "â”‚Â Â  â””â”€â”€ labels\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_456.txt\n",
        "â”œâ”€â”€ test\n",
        "â”‚Â Â  â”œâ”€â”€ images\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_789.jpg\n",
        "â”‚Â Â  â””â”€â”€ labels\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_789.txt\n",
        "â”œâ”€â”€ data.yaml\n",
        "â”œâ”€â”€ README.roboflow.txt\n",
        "```\n",
        "\n",
        "![](https://robflow-public-assets.s3.amazonaws.com/how-to-train-yolov5-segmentation-format.gif)\n",
        "\n",
        "\n",
        "Copy and paste that snippet into the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtqZdTl4GG8N"
      },
      "outputs": [],
      "source": [
        "# REPLACE the below with your exported code snippet from above\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
        "project = rf.workspace(\"paul-guerrie-tang1\").project(\"asl-poly-instance-seg\")\n",
        "dataset = project.version(24).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZwzVuLTtpoI"
      },
      "source": [
        "### Train On Custom Data ðŸŽ‰\n",
        "Here, we use the `dataset.location` attribute to pass our dataset to the `--data` parameter.\n",
        "\n",
        "Note: we're training for 100 epochs here. We're also starting training from the pretrained weights. Larger datasets will likely benefit from longer training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSBVTWnAMU0k"
      },
      "outputs": [],
      "source": [
        "!python segment/train.py --img 320 --batch 128 --epochs 100 --data {dataset.location}/data.yaml --weights yolov5s-seg.pt\n",
        "display.Image(filename=f'runs/train-seg/exp/results.png', width=1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8gmSaAtpoJ"
      },
      "source": [
        "Above we see training plots for our custom trained model. This plot along with other detailed results and sample predictions are all automatically generated and stored in the `runs` folder. See `runs/train-seg/exp`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZjIfCzftpoJ"
      },
      "source": [
        "### Validate Your Custom Model\n",
        "\n",
        "Repeat step 2 from above to test and validate your custom model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6DFBei-tpoJ"
      },
      "outputs": [],
      "source": [
        "!python segment/val.py --weights runs/train-seg/exp/weights/best.pt --data {dataset.location}/data.yaml --img 320"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4SGEO0VtpoJ"
      },
      "source": [
        "### Infer With Your Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abjYoz8itpoJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#Get the path of an image from the test or validation set\n",
        "if os.path.exists(os.path.join(dataset.location, \"test\")):\n",
        "  split_path = os.path.join(dataset.location, \"test\", \"images\")\n",
        "else:\n",
        "  split_path = os.path.join(dataset.location, \"valid\", \"images\")\n",
        "example_image_name = os.listdir(split_path)[0]\n",
        "example_image_path = os.path.join(split_path, example_image_name)\n",
        "\n",
        "#Infer\n",
        "!python segment/predict.py --img 320 --weights runs/train-seg/exp/weights/best.pt --source {example_image_path}\n",
        "display.Image(filename=f'runs/predict-seg/exp2/{example_image_name}', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIbCkxfPtpoJ"
      },
      "source": [
        "The image above shows the resulting prediction overlayed on the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ird2cKXGtpoK"
      },
      "source": [
        "## (OPTIONAL) Improve Our Model with Active Learning\n",
        "\n",
        "Now that we've trained our model once, we will want to continue to improve its performance. Improvement is largely dependent on improving our dataset.\n",
        "\n",
        "We can programmatically upload example failure images back to our custom dataset based on conditions (like seeing an underrpresented class or a low confidence score) using the same `pip` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw-LVMEatpoK"
      },
      "outputs": [],
      "source": [
        "#Prepare Our Example Image Directory\n",
        "#To run on your own images, replace the directory on the line below with a\n",
        "#directory you provide (by uploading to the colab environment, for example)\n",
        "example_image_dir = \"../example_images\"\n",
        "\n",
        "#We are going to download example images from the web for the purposes of this\n",
        "#demo. These images are relevant to the ASL Poly dataset. Skip the rest of this\n",
        "#cell if you are providing your own example image directory.\n",
        "os.makedirs(example_image_dir, exist_ok=True)\n",
        "image_links = [\n",
        "    \"https://i.imgur.com/rFsDnHC.jpg\",\n",
        "    \"https://i.imgur.com/aEcceXm.jpg\",\n",
        "    \"https://i.imgur.com/s4N63fx.jpg\",\n",
        "    ]\n",
        "\n",
        "for i,link in enumerate(image_links):\n",
        "  img_data = requests.get(link).content\n",
        "  with open(os.path.join(example_image_dir,f'example_{i}.jpg'), 'wb') as handler:\n",
        "    handler.write(img_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer On Directory\n",
        "!python segment/predict.py --img 320 --weights runs/train-seg/exp/weights/best.pt --source {example_image_dir} --save-txt --save-conf"
      ],
      "metadata": {
        "id": "yq46nfIk8s7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Results Files and Conditionally Upload\n",
        "\n",
        "#If my model has a confidence of less than 80% for a prediction, let's help it\n",
        "#out by uploading this image back to our dataset. Then we can add a ground truth\n",
        "#label to it so that it will be included in our next training run and future\n",
        "#prediction results will improve.\n",
        "MIN_CONF_THRESHOLD = 0.8\n",
        "\n",
        "for i,txt_file in enumerate(os.listdir(\"runs/predict-seg/exp3/labels\")):\n",
        "  with open(os.path.join(\"runs/predict-seg/exp3/labels\",txt_file), 'r') as fid:\n",
        "    for line in fid:\n",
        "      label, x1, y1, x2, y2, conf = line.split(\" \")\n",
        "      conf = float(conf)\n",
        "      if conf < MIN_CONF_THRESHOLD:\n",
        "        print(f\"Image has a low confidence prediction, uploading to project: example_{i}.jpg\")\n",
        "        #Upload via Roboflow pip package\n",
        "        project.upload(os.path.join(example_image_dir,f'example_{i}.jpg'))\n",
        "        break"
      ],
      "metadata": {
        "id": "im54PYtA-A_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}