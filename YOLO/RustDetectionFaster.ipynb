{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script is designed for training, validating, and testing a Faster R-CNN model to detect rust in images. It utilizes a COCO-style dataset and incorporates essential components for dataset preprocessing, model setup, and evaluation. The script also includes visualization capabilities to render bounding box predictions on test images.\n",
    "\n",
    "The preprocessing stage involves converting bounding box formats from [x, y, width, height] to [x_min, y_min, x_max, y_max] for compatibility with Faster R-CNN. A custom dataset class CocoDetectionAdjusted inherits from torchvision.datasets.CocoDetection to adjust bounding boxes dynamically during data loading. Additionally, a collate_fn function is provided to handle batching while managing empty or invalid annotations gracefully.\n",
    "\n",
    "The model is initialized using Faster R-CNN with a ResNet-50 backbone pre-trained on ImageNet. The get_model function modifies the prediction head to output the desired number of classes based on the dataset. Optimizer parameters include stochastic gradient descent (SGD) with momentum and weight decay, along with a learning rate scheduler (StepLR) to adapt the learning rate during training.\n",
    "\n",
    "The train_model function is the core of the training process, iterating over multiple epochs while logging training and validation losses. Validation loss is calculated using the validate_model function, which evaluates the model on the validation dataset. The script also saves the trained model weights to a file for future use. A plot of training and validation loss is generated at the end to visualize the model's performance.\n",
    "\n",
    "Testing involves the evaluate_model function, which predicts bounding boxes on the test dataset. The visualize_predictions function overlays bounding boxes and confidence scores onto test images, saving these visualized images into directories categorized as rust_detected and non_rust. This provides a clear understanding of the model's predictions.\n",
    "\n",
    "To use the script, prepare a COCO-style dataset with separate directories for training, validation, and testing images, along with corresponding annotation JSON files. Update the dataset_dir variable to point to the dataset location. Running the script trains the model and evaluates its performance on the test set, generating predictions and saving results.\n",
    "\n",
    "The output includes a trained model saved as fine_tuned_rust_detection.pth, visualized predictions in designated directories, and a plot of training and validation losses. For enhancements, consider adding test accuracy tracking, early stopping, and data augmentation for improved robustness and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 709/2368 [06:42<15:41,  1.76it/s, Batch Loss=0.459]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m         plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# === Run Training, Validation, and Testing ===\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m evaluate_model(model, test_loader, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 101\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[0;32m     98\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 101\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Update progress bar with batch loss\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "def convert_bbox_format(bbox_list):\n",
    "    \"\"\"Convert bounding boxes from [x, y, width, height] to [x_min, y_min, x_max, y_max].\"\"\"\n",
    "    return [[x, y, x + w, y + h] for x, y, w, h in bbox_list]\n",
    "\n",
    "class CocoDetectionAdjusted(CocoDetection):\n",
    "    \"\"\"Custom dataset class with bounding box conversion.\"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "\n",
    "        for t in target:\n",
    "            if \"bbox\" in t:\n",
    "                t[\"bbox\"] = convert_bbox_format([t[\"bbox\"]])[0]\n",
    "\n",
    "        boxes = torch.tensor([t[\"bbox\"] for t in target], dtype=torch.float32)\n",
    "        labels = torch.tensor([t[\"category_id\"] for t in target], dtype=torch.int64)\n",
    "\n",
    "        if len(boxes) == 0 or len(labels) == 0:\n",
    "            return img, {\"boxes\": torch.empty((0, 4), dtype=torch.float32), \"labels\": torch.empty((0,), dtype=torch.int64)}\n",
    "\n",
    "        return img, {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle empty images or targets.\"\"\"\n",
    "    batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# === Paths ===\n",
    "dataset_dir = \"C:\\\\Users\\\\braya\\\\Downloads\\\\ECS174_Project\\\\dataset\"\n",
    "train_image_dir, train_ann_file = os.path.join(dataset_dir, \"train/images\"), os.path.join(dataset_dir, \"train/_annotations.coco.json\")\n",
    "valid_image_dir, valid_ann_file = os.path.join(dataset_dir, \"valid/images\"), os.path.join(dataset_dir, \"valid/_annotations.coco.json\")\n",
    "test_image_dir, test_ann_file = os.path.join(dataset_dir, \"test/images\"), os.path.join(dataset_dir, \"test/_annotations.coco.json\")\n",
    "\n",
    "# === Data Loaders ===\n",
    "def get_dataloader(image_dir, ann_file, batch_size, shuffle):\n",
    "    dataset = CocoDetectionAdjusted(root=image_dir, annFile=ann_file, transform=Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = get_dataloader(train_image_dir, train_ann_file, batch_size=4, shuffle=True)\n",
    "valid_loader = get_dataloader(valid_image_dir, valid_ann_file, batch_size=4, shuffle=False)\n",
    "test_loader = get_dataloader(test_image_dir, test_ann_file, batch_size=6, shuffle=False)\n",
    "\n",
    "# === Model Setup ===\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model(num_classes=11)  # Update this to match your dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# === Training Function ===\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=2):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in progress_bar:\n",
    "            if images is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = losses.item()\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # Update progress bar with batch loss\n",
    "            progress_bar.set_postfix({\"Batch Loss\": batch_loss})\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        training_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validate after each epoch\n",
    "        validation_loss = validate_model(model, valid_loader)\n",
    "        validation_losses.append(validation_loss)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"fine_tuned_rust_detection.pth\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(1, num_epochs + 1), validation_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# === Validation Function ===\n",
    "def validate_model(model, valid_loader):\n",
    "    model.train()\n",
    "    validation_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(valid_loader, desc=\"Validating\"):\n",
    "            if images is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            validation_loss += losses.item()\n",
    "\n",
    "    return validation_loss\n",
    "\n",
    "# === Testing Function ===\n",
    "def evaluate_model(model, loader, save_dir, rust_label=10, confidence_threshold=0.5):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Testing\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            predictions = model(images)\n",
    "            visualize_predictions(images, predictions, save_dir, rust_label, confidence_threshold)\n",
    "\n",
    "def visualize_predictions(images, predictions, save_dir, rust_label=10, confidence_threshold=0.5):\n",
    "    rust_dir, non_rust_dir = os.path.join(save_dir, \"rust_detected\"), os.path.join(save_dir, \"non_rust\")\n",
    "    os.makedirs(rust_dir, exist_ok=True)\n",
    "    os.makedirs(non_rust_dir, exist_ok=True)\n",
    "\n",
    "    for idx, (img, prediction) in enumerate(zip(images, predictions)):\n",
    "        img = img.permute(1, 2, 0).cpu().numpy()\n",
    "        img = np.clip(img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406], 0, 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(img)\n",
    "\n",
    "        rust_found = False\n",
    "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
    "            if label == rust_label and score >= confidence_threshold:\n",
    "                rust_found = True\n",
    "                x_min, y_min, x_max, y_max = box.cpu().numpy()\n",
    "                rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
    "                plt.gca().add_patch(rect)\n",
    "                plt.text(x_min, y_min - 10, f\"{score:.2f}\", color=\"red\", fontsize=8)\n",
    "\n",
    "        save_path = os.path.join(rust_dir if rust_found else non_rust_dir, f\"image_{idx}.png\")\n",
    "        Image.fromarray((img * 255).astype(np.uint8)).save(save_path)\n",
    "        plt.savefig(save_path.replace(\".png\", \"_visualized.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# === Run Training, Validation, and Testing ===\n",
    "train_model(model, train_loader, valid_loader, num_epochs=2)\n",
    "evaluate_model(model, test_loader, save_dir=\"test_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: \n",
    "The training process for the fine-tuned Faster R-CNN model on the rust detection dataset spanned 10 epochs. During the first epoch, the training loss started at a high value of 1646.47, which decreased significantly to 771.72 by the tenth epoch. Validation loss showed a consistent reduction over the initial epochs, beginning at 36.17 in the first epoch and gradually reducing to 14.17 by the fifth epoch. However, after the fifth epoch, the validation loss began to increase slightly, reaching 20.78 by the tenth epoch. This suggests a potential overfitting trend as the model continues to learn from the training data, but generalization on validation data starts to decline. The training process indicates that the model effectively learns the rust detection task initially, but further optimization or regularization techniques might be necessary to maintain better generalization as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated code: The updated script incorporates test accuracy tracking during the training process and reduces the number of epochs from 10 to 5 for faster experimentation and evaluation. The training function now calculates and records test accuracy at the end of each epoch, alongside training and validation losses. These metrics are plotted together on a graph to provide a comprehensive view of the model's performance over the training period. The test accuracy tracking allows better insight into how well the model generalizes to unseen data, while reducing the number of epochs saves computational resources and time. This update ensures a more efficient and informative training process, balancing performance evaluation with runtime considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/YOLO\n",
      "PRO TIP üí° Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "Model loaded successfully and is running on mps\n",
      "\n",
      "=== Starting Training ===\n",
      "New https://pypi.org/project/ultralytics/8.3.49 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5s.pt, data=./dataset/data.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=mps:0, workers=8, project=None, name=train13, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train13\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'dataset/data.yaml' error ‚ùå \nDataset 'dataset/data.yaml' images not found ‚ö†Ô∏è, missing path '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets/dataset/valid/images'\nNote dataset download directory is '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets'. You can update this in '/Users/harjotgill/Library/Application Support/Ultralytics/settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/intro--cv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:562\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    561\u001b[0m }:\n\u001b[0;32m--> 562\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/intro--cv/lib/python3.12/site-packages/ultralytics/data/utils.py:329\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    328\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[1;32m    330\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \nDataset 'dataset/data.yaml' images not found ‚ö†Ô∏è, missing path '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets/dataset/valid/images'\nNote dataset download directory is '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets'. You can update this in '/Users/harjotgill/Library/Application Support/Ultralytics/settings.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo loss file found for plotting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# === Run Training and Testing ===\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_yaml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m validate_model(model, data_yaml\u001b[38;5;241m=\u001b[39mdata_yaml)\n\u001b[1;32m     85\u001b[0m test_model(model, image_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/images\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_yaml, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train YOLOv5 on the specified dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Starting Training ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/intro--cv/lib/python3.12/site-packages/ultralytics/engine/model.py:799\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    797\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m--> 799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/intro--cv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:133\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/intro--cv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:566\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset 'dataset/data.yaml' error ‚ùå \nDataset 'dataset/data.yaml' images not found ‚ö†Ô∏è, missing path '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets/dataset/valid/images'\nNote dataset download directory is '/Users/harjotgill/Desktop/UC_DAVIS/ECS174/ECS174_Project/yolov5/datasets'. You can update this in '/Users/harjotgill/Library/Application Support/Ultralytics/settings.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO  # YOLOv5 framework\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "def convert_to_yolo_format(bbox_list, image_width, image_height):\n",
    "    \"\"\"Convert bounding boxes from [x_min, y_min, x_max, y_max] to YOLO format [x_center, y_center, width, height].\"\"\"\n",
    "    yolo_bboxes = []\n",
    "    for x_min, y_min, x_max, y_max in bbox_list:\n",
    "        x_center = (x_min + x_max) / 2 / image_width\n",
    "        y_center = (y_min + y_max) / 2 / image_height\n",
    "        width = (x_max - x_min) / image_width\n",
    "        height = (y_max - y_min) / image_height\n",
    "        yolo_bboxes.append([x_center, y_center, width, height])\n",
    "    return yolo_bboxes\n",
    "\n",
    "# === Paths ===\n",
    "\n",
    "# save output of pwd to variable\n",
    "current_directory = os.getcwd()\n",
    "dataset_dir = os.path.join(current_directory, \"dataset\")\n",
    "data_yaml = os.path.join(dataset_dir, \"data.yaml\")\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "!pwd \n",
    "data_yaml = \"./dataset/data.yaml\" # YOLO format data.yaml file\n",
    "\n",
    "# === Model Setup ===\n",
    "def get_model():\n",
    "    \"\"\"Load the YOLO model.\"\"\"\n",
    "    model = YOLO('yolov5s.pt')  # Pretrained YOLOv5s model\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully and is running on\", device)\n",
    "\n",
    "# === Training ===\n",
    "def train_model(model, data_yaml, num_epochs=50, batch_size=8):\n",
    "    \"\"\"Train YOLOv5 on the specified dataset.\"\"\"\n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    model.train(data=data_yaml, epochs=num_epochs, batch=batch_size)\n",
    "\n",
    "# === Validation ===\n",
    "def validate_model(model, data_yaml):\n",
    "    \"\"\"Validate YOLOv5 on the validation dataset.\"\"\"\n",
    "    print(\"\\n=== Validating Model ===\")\n",
    "    results = model.val(data=data_yaml)\n",
    "    print(f\"Validation Results: {results}\")\n",
    "    return results\n",
    "\n",
    "# === Testing ===\n",
    "def test_model(model, image_dir):\n",
    "    \"\"\"Run YOLOv5 inference on test images.\"\"\"\n",
    "    print(\"\\n=== Testing Model ===\")\n",
    "    results = model.predict(source=image_dir)\n",
    "    for result in results:\n",
    "        print(result)  # Prints bounding boxes, class labels, and confidence scores\n",
    "    return results\n",
    "\n",
    "# === Loss Plotting (Optional) ===\n",
    "def plot_loss_curve(log_dir):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    loss_file = os.path.join(log_dir, \"results.csv\")\n",
    "    if os.path.exists(loss_file):\n",
    "        import pandas as pd\n",
    "        data = pd.read_csv(loss_file)\n",
    "        plt.plot(data[\"epoch\"], data[\"train/box_loss\"], label=\"Box Loss\")\n",
    "        plt.plot(data[\"epoch\"], data[\"train/obj_loss\"], label=\"Objectness Loss\")\n",
    "        plt.plot(data[\"epoch\"], data[\"train/cls_loss\"], label=\"Classification Loss\")\n",
    "        plt.plot(data[\"epoch\"], data[\"val/box_loss\"], label=\"Validation Box Loss\", linestyle=\"--\")\n",
    "        plt.plot(data[\"epoch\"], data[\"val/obj_loss\"], label=\"Validation Objectness Loss\", linestyle=\"--\")\n",
    "        plt.plot(data[\"epoch\"], data[\"val/cls_loss\"], label=\"Validation Classification Loss\", linestyle=\"--\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No loss file found for plotting.\")\n",
    "\n",
    "# === Run Training and Testing ===\n",
    "train_model(model, data_yaml=data_yaml, num_epochs=10)\n",
    "validate_model(model, data_yaml=data_yaml)\n",
    "test_model(model, image_dir=os.path.join(dataset_dir, \"test/images\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro--cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
